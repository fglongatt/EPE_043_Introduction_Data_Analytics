# EPE 043 Introduction Data_Analytics for Smart Energy Systems
## Objective
The aims of this module are to:
Introduce the main principles of diagnostic, predictive and operational analytics with relevance to the energy sector.
Cover the key theoretical principles under-pinning big data analytics
Utilise case studies to explore how stakeholders such as electricity system operators, utilities and aggregators can use data analytics to create clean, affordable, and flexible energy systems.
## 


## Content:
* Introduction to big data: challenges and opportunities for the energy sector
* Applications and analysis of Time-series data
* Predictive and diagnostic analytics: probabilistic models, statistical models and machine learning (ML)
* Practical tools for analytics applications in energy contexts
* Architectures for big data
* Visualisation tools for big data systems
* Applied case studies


## Chapter 2: Descriptive Statistics
_Statistics_ is the discipline that concerns the collection, organisation, analysis, interpretation, and presentation of data.
Summary statistics are used to summarise a set of observations to communicate the largest amount of information as simply as possible.
A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarises features from a collection of information, while descriptive statistics (in the mass noun sense) is the process of using and analysing those statistics. 

Read more at: https://www.researchgate.net/publication/389059945_Introduction_to_Data_Analytics_Chapter_2_Descriptive_Statistics

## Chapter 3: Probability Function
In statistics, a _population_ is defined as the complete set of all possible observations or measurements that can be made about a specific characteristic within a group or category. It includes every single item or individual that fits the criteria of interest.
A _sample_ is a subset of individuals, items, or data points selected from a larger population. The purpose of using a sample is to gather information and make inferences about the entire population without examining every single member of that population.
The _population_ is the entire set or group to conclude about, whereas the sample is the specific group from which data is collected.


## Chapter 4: Data Visualisation 
One of the most valuable exercises when exploring new datasets is plotting the data somehow. _Data visualisation_ is the graphical representation of information and data. Using visual elements like charts, graphs, and maps, data visualisation tools provide an accessible way to see and understand trends, outliers, and patterns in data.
The benefits of data visualisation include:
* Improved comprehension: Visuals make complex data easier to understand.
* Quick insights: Patterns and trends become apparent quickly.
*	Better communication: Visuals can convey information more effectively than text alone.
*	Enhanced decision-making: Clear visuals can support better analysis and informed decisions.
Read more at: https://www.researchgate.net/publication/389064233_Introduction_to_Data_Analytics_Chapter_4_Data_Visualisation

## Chapter 5: Bayes Theorem
Baye’s theorem, named after 18th-century British mathematician Thomas Bayes (1701-1761), is a mathematical formula for determining conditional probability.
Bayes’s theorem is very important in statistical inference and describes the probability of an event based on prior knowledge related to that event or measurement. 
Baye’s theorem that the probability of an event occurring, given that another event has already occurred, is equal to a formula that includes conditional probabilities.

## Chapter 6: Linear Regression
_Regression_ is a fascinating statistical technique for analysing the relationship between variables. It helps us understand how the dependent variable changes when any one of the independent variables is varied.
Linear regression is a simple yet powerful statistical method for understanding the relationship between two continuous variables. It builds a model of the relationship between two or more variables.
Initially, consider the case of two variables, and then the process can be generalised in multiple linear regression. 

## Chapter 7: Hypothesis Testing
_Hypothesis testing_ is a fundamental concept in statistics used to determine if there is enough evidence to reject a null hypothesis. Hypothesis testing regarding sampling usually involves using sample data to make inferences about a population, so the main concern is using sampling to test some particular hypothesis about the population. 

## Chapter 8: Monte Carlo Methods
_Monte Carlo methods_ (MCMs) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. 
The modern Monte Carlo method was developed during the Manhattan Project at Los Alamos National Laboratory. 
Physicists and mathematicians like Enrico Fermi, Stanisław Ulam, John von Neumann, and Nicholas Metropolis were working on simulating neutron diffusion in nuclear chain reactions. The name “Monte Carlo” was inspired by Stanisław Ulam’s uncle, who would borrow money by saying he was going to the Monte Carlo Casino in Monaco. Ulam thought the randomness in gambling was similar to the randomness needed in their simulations. 
Monte Carlo simulations were used to model and predict the behaviour of neutrons within a nuclear chain reaction, allowing scientists to better understand and design atomic bombs by simulating the complex, probabilistic nature of nuclear fission, especially when dealing with uncertainties in variables like neutron scattering and critical mass, which were difficult to test directly in real-world experiments; essentially providing a way to analyse a wide range of possible outcomes based on random sampling. These methods allowed the scientists to cover the multidimensionality of the problem without the limits of numerical methods. With the help of early computers like ENIAC and MANIAC, the team used Monte Carlo methods to obtain more accurate predictions.


## Chapter 9:




